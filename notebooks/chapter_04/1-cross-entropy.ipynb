{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "import gym\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "MAX_ITER = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features:int, hidden_size:int, num_actions:int):\n",
    "        super(Net, self).__init__()\n",
    "        # Define two linear layers\n",
    "        self.linear1 = nn.Linear(in_features=num_features, out_features=hidden_size)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_size, out_features=num_actions)\n",
    "\n",
    "        # Define a ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)  # First linear layer\n",
    "        x = self.relu(x)     # ReLU activation\n",
    "        x = self.linear2(x)  # Second linear layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['total_rewards', 'steps'])\n",
    "episodeStep = namedtuple('episodeStep', field_names=['state', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_episode(env:gym.Env, net:Net):\n",
    "    net.eval()\n",
    "    total_rewards = 0\n",
    "    steps = []\n",
    "    sm = nn.Softmax(dim=0)\n",
    "    \n",
    "    current_state = env.reset()[0]\n",
    "    \n",
    "    while True:\n",
    "        current_state_tensor = torch.FloatTensor(current_state)\n",
    "        action_prob = sm(net(current_state_tensor))\n",
    "        action_prob = action_prob.detach().numpy()\n",
    "        action = np.random.choice(env.action_space.n, p=action_prob)\n",
    "        \n",
    "        next_state, reward, terminated, _, info = env.step(action=action)\n",
    "        total_rewards += reward\n",
    "        current_step = episodeStep(state=current_state, action=action)\n",
    "        steps.append(current_step)\n",
    "        \n",
    "        if terminated:\n",
    "            e = Episode(total_rewards=total_rewards, steps=steps)\n",
    "            return e\n",
    "            \n",
    "        \n",
    "        current_state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(env:gym.Env, net:Net, batch_size:int):\n",
    "    batch = []\n",
    "    for i in range(batch_size):\n",
    "        episode = create_episode(env, net)\n",
    "        batch.append(episode)\n",
    "        \n",
    "    return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s:s.total_rewards, batch))\n",
    "    reward_percentile = np.percentile(rewards, percentile)\n",
    "    mean_reward = np.mean(rewards)\n",
    "    \n",
    "    training_states = []\n",
    "    training_actions = []\n",
    "    for total_rewards, steps in batch:\n",
    "        if total_rewards< reward_percentile:\n",
    "            continue\n",
    "        training_states.extend(map(lambda step: step.state, steps))\n",
    "        training_actions.extend(map(lambda step: step.action, steps))\n",
    "        \n",
    "    return training_states, training_actions, reward_percentile, mean_reward\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0, mean reward: 30.5\n",
      "Current iteration: 1, mean reward: 28.0\n",
      "Current iteration: 2, mean reward: 41.0\n",
      "Current iteration: 3, mean reward: 29.0\n",
      "Current iteration: 4, mean reward: 30.5\n",
      "Current iteration: 5, mean reward: 49.5\n",
      "Current iteration: 6, mean reward: 40.5\n",
      "Current iteration: 7, mean reward: 55.0\n",
      "Current iteration: 8, mean reward: 61.5\n",
      "Current iteration: 9, mean reward: 68.0\n",
      "Current iteration: 10, mean reward: 80.0\n",
      "Current iteration: 11, mean reward: 60.5\n",
      "Current iteration: 12, mean reward: 81.5\n",
      "Current iteration: 13, mean reward: 89.0\n",
      "Current iteration: 14, mean reward: 97.5\n",
      "Current iteration: 15, mean reward: 93.0\n",
      "Current iteration: 16, mean reward: 104.0\n",
      "Current iteration: 17, mean reward: 155.0\n",
      "Current iteration: 18, mean reward: 117.0\n",
      "Current iteration: 19, mean reward: 105.0\n",
      "Current iteration: 20, mean reward: 145.5\n",
      "Current iteration: 21, mean reward: 103.5\n",
      "Current iteration: 22, mean reward: 104.0\n",
      "Current iteration: 23, mean reward: 156.0\n",
      "Current iteration: 24, mean reward: 166.5\n",
      "Current iteration: 25, mean reward: 168.5\n",
      "Current iteration: 26, mean reward: 189.5\n",
      "Current iteration: 27, mean reward: 179.5\n",
      "Current iteration: 28, mean reward: 125.0\n",
      "Current iteration: 29, mean reward: 143.0\n",
      "Current iteration: 30, mean reward: 154.5\n",
      "Current iteration: 31, mean reward: 138.0\n",
      "Current iteration: 32, mean reward: 138.0\n",
      "Current iteration: 33, mean reward: 159.5\n",
      "Current iteration: 34, mean reward: 155.5\n",
      "Current iteration: 35, mean reward: 225.5\n",
      "Current iteration: 36, mean reward: 247.5\n",
      "Current iteration: 37, mean reward: 275.5\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "    num_features = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "    net = Net(num_features=num_features, hidden_size=HIDDEN_SIZE, num_actions=num_actions)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(params = net.parameters(), lr = 0.01)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir='cart_pole_tb')\n",
    "    for i in range(MAX_ITER):\n",
    "        batch = create_batch(env, net, BATCH_SIZE)\n",
    "        training_states, training_actions, reward_percentile, mean_reward = filter_batch(batch=batch, percentile=PERCENTILE)\n",
    "        training_states = torch.FloatTensor(training_states)\n",
    "        training_actions = torch.LongTensor(training_actions)\n",
    "        # Zero the parameter gradients\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(training_states)\n",
    "        outputs = outputs.float()\n",
    "        loss = criterion(outputs, training_actions)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        print('Current iteration: {}, mean reward: {}'.format(i, mean_reward))\n",
    "        \n",
    "        writer.add_scalar(\"loss\", loss.item(), i)\n",
    "        writer.add_scalar(\"reward_bound\", reward_percentile, i)\n",
    "        writer.add_scalar(\"mean_reward\", mean_reward, i)\n",
    "        if mean_reward > 199:\n",
    "            break\n",
    "        \n",
    "    writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CartPole-v0\")\n",
    "# current_state = env.reset()[0]\n",
    "# current_state = torch.FloatTensor(current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = env.observation_space.shape[0]\n",
    "# num_actions = env.action_space.n\n",
    "# net = Net(num_features=num_features, hidden_size=HIDDEN_SIZE, num_actions=num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
