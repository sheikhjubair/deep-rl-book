{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "import gym\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "MAX_ITER = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space,\n",
    "                          gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n, )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            0.0, 1.0, shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features:int, hidden_size:int, num_actions:int):\n",
    "        super(Net, self).__init__()\n",
    "        # Define two linear layers\n",
    "        self.linear1 = nn.Linear(in_features=num_features, out_features=hidden_size)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_size, out_features=num_actions)\n",
    "\n",
    "        # Define a ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)  # First linear layer\n",
    "        x = self.relu(x)     # ReLU activation\n",
    "        x = self.linear2(x)  # Second linear layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['total_rewards', 'steps'])\n",
    "episodeStep = namedtuple('episodeStep', field_names=['state', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_episode(env:gym.Env, net:Net):\n",
    "    net.eval()\n",
    "    total_rewards = 0\n",
    "    steps = []\n",
    "    sm = nn.Softmax(dim=0)\n",
    "    \n",
    "    current_state = env.reset()[0]\n",
    "    \n",
    "    while True:\n",
    "        current_state_tensor = torch.FloatTensor(current_state)\n",
    "        action_prob = sm(net(current_state_tensor))\n",
    "        action_prob = action_prob.detach().numpy()\n",
    "        action = np.random.choice(env.action_space.n, p=action_prob)\n",
    "        \n",
    "        next_state, reward, terminated, _, info = env.step(action=action)\n",
    "        total_rewards += reward\n",
    "        current_step = episodeStep(state=current_state, action=action)\n",
    "        steps.append(current_step)\n",
    "        \n",
    "        if terminated:\n",
    "            e = Episode(total_rewards=total_rewards, steps=steps)\n",
    "            return e\n",
    "            \n",
    "        \n",
    "        current_state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(env:gym.Env, net:Net, batch_size:int):\n",
    "    batch = []\n",
    "    for i in range(batch_size):\n",
    "        episode = create_episode(env, net)\n",
    "        batch.append(episode)\n",
    "        \n",
    "    return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s:s.total_rewards, batch))\n",
    "    reward_percentile = np.percentile(rewards, percentile)\n",
    "    mean_reward = np.mean(rewards)\n",
    "    \n",
    "    training_states = []\n",
    "    training_actions = []\n",
    "    for total_rewards, steps in batch:\n",
    "        if total_rewards< reward_percentile:\n",
    "            continue\n",
    "        training_states.extend(map(lambda step: step.state, steps))\n",
    "        training_actions.extend(map(lambda step: step.action, steps))\n",
    "        \n",
    "    return training_states, training_actions, reward_percentile, mean_reward\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0, mean reward: 0.0\n",
      "Current iteration: 1, mean reward: 0.0\n",
      "Current iteration: 2, mean reward: 0.0\n",
      "Current iteration: 3, mean reward: 0.0\n",
      "Current iteration: 4, mean reward: 0.0625\n",
      "Current iteration: 5, mean reward: 0.0625\n",
      "Current iteration: 6, mean reward: 0.0\n",
      "Current iteration: 7, mean reward: 0.0\n",
      "Current iteration: 8, mean reward: 0.0\n",
      "Current iteration: 9, mean reward: 0.0\n",
      "Current iteration: 10, mean reward: 0.0625\n",
      "Current iteration: 11, mean reward: 0.0\n",
      "Current iteration: 12, mean reward: 0.0\n",
      "Current iteration: 13, mean reward: 0.0\n",
      "Current iteration: 14, mean reward: 0.0\n",
      "Current iteration: 15, mean reward: 0.0625\n",
      "Current iteration: 16, mean reward: 0.0\n",
      "Current iteration: 17, mean reward: 0.0\n",
      "Current iteration: 18, mean reward: 0.0\n",
      "Current iteration: 19, mean reward: 0.0625\n",
      "Current iteration: 20, mean reward: 0.0625\n",
      "Current iteration: 21, mean reward: 0.0\n",
      "Current iteration: 22, mean reward: 0.0\n",
      "Current iteration: 23, mean reward: 0.0\n",
      "Current iteration: 24, mean reward: 0.0\n",
      "Current iteration: 25, mean reward: 0.0625\n",
      "Current iteration: 26, mean reward: 0.0\n",
      "Current iteration: 27, mean reward: 0.0625\n",
      "Current iteration: 28, mean reward: 0.0625\n",
      "Current iteration: 29, mean reward: 0.0625\n",
      "Current iteration: 30, mean reward: 0.0\n",
      "Current iteration: 31, mean reward: 0.0625\n",
      "Current iteration: 32, mean reward: 0.0625\n",
      "Current iteration: 33, mean reward: 0.0\n",
      "Current iteration: 34, mean reward: 0.0\n",
      "Current iteration: 35, mean reward: 0.125\n",
      "Current iteration: 36, mean reward: 0.0625\n",
      "Current iteration: 37, mean reward: 0.0\n",
      "Current iteration: 38, mean reward: 0.0\n",
      "Current iteration: 39, mean reward: 0.125\n",
      "Current iteration: 40, mean reward: 0.125\n",
      "Current iteration: 41, mean reward: 0.125\n",
      "Current iteration: 42, mean reward: 0.0\n",
      "Current iteration: 43, mean reward: 0.0\n",
      "Current iteration: 44, mean reward: 0.0625\n",
      "Current iteration: 45, mean reward: 0.0\n",
      "Current iteration: 46, mean reward: 0.0\n",
      "Current iteration: 47, mean reward: 0.125\n",
      "Current iteration: 48, mean reward: 0.125\n",
      "Current iteration: 49, mean reward: 0.0625\n",
      "Current iteration: 50, mean reward: 0.0625\n",
      "Current iteration: 51, mean reward: 0.125\n",
      "Current iteration: 52, mean reward: 0.0625\n",
      "Current iteration: 53, mean reward: 0.0625\n",
      "Current iteration: 54, mean reward: 0.0\n",
      "Current iteration: 55, mean reward: 0.0\n",
      "Current iteration: 56, mean reward: 0.0625\n",
      "Current iteration: 57, mean reward: 0.0\n",
      "Current iteration: 58, mean reward: 0.0\n",
      "Current iteration: 59, mean reward: 0.0\n",
      "Current iteration: 60, mean reward: 0.0\n",
      "Current iteration: 61, mean reward: 0.0\n",
      "Current iteration: 62, mean reward: 0.0625\n",
      "Current iteration: 63, mean reward: 0.0\n",
      "Current iteration: 64, mean reward: 0.0625\n",
      "Current iteration: 65, mean reward: 0.0625\n",
      "Current iteration: 66, mean reward: 0.0625\n",
      "Current iteration: 67, mean reward: 0.0\n",
      "Current iteration: 68, mean reward: 0.0625\n",
      "Current iteration: 69, mean reward: 0.0\n",
      "Current iteration: 70, mean reward: 0.0625\n",
      "Current iteration: 71, mean reward: 0.0\n",
      "Current iteration: 72, mean reward: 0.0\n",
      "Current iteration: 73, mean reward: 0.0\n",
      "Current iteration: 74, mean reward: 0.0\n",
      "Current iteration: 75, mean reward: 0.0\n",
      "Current iteration: 76, mean reward: 0.25\n",
      "Current iteration: 77, mean reward: 0.125\n",
      "Current iteration: 78, mean reward: 0.125\n",
      "Current iteration: 79, mean reward: 0.0\n",
      "Current iteration: 80, mean reward: 0.0625\n",
      "Current iteration: 81, mean reward: 0.0\n",
      "Current iteration: 82, mean reward: 0.0\n",
      "Current iteration: 83, mean reward: 0.0\n",
      "Current iteration: 84, mean reward: 0.0\n",
      "Current iteration: 85, mean reward: 0.0\n",
      "Current iteration: 86, mean reward: 0.0\n",
      "Current iteration: 87, mean reward: 0.0\n",
      "Current iteration: 88, mean reward: 0.0625\n",
      "Current iteration: 89, mean reward: 0.0625\n",
      "Current iteration: 90, mean reward: 0.0\n",
      "Current iteration: 91, mean reward: 0.0\n",
      "Current iteration: 92, mean reward: 0.0625\n",
      "Current iteration: 93, mean reward: 0.0\n",
      "Current iteration: 94, mean reward: 0.0625\n",
      "Current iteration: 95, mean reward: 0.0625\n",
      "Current iteration: 96, mean reward: 0.0\n",
      "Current iteration: 97, mean reward: 0.0\n",
      "Current iteration: 98, mean reward: 0.0\n",
      "Current iteration: 99, mean reward: 0.0\n",
      "Current iteration: 100, mean reward: 0.0\n",
      "Current iteration: 101, mean reward: 0.0\n",
      "Current iteration: 102, mean reward: 0.0\n",
      "Current iteration: 103, mean reward: 0.0\n",
      "Current iteration: 104, mean reward: 0.0\n",
      "Current iteration: 105, mean reward: 0.0\n",
      "Current iteration: 106, mean reward: 0.0\n",
      "Current iteration: 107, mean reward: 0.0625\n",
      "Current iteration: 108, mean reward: 0.0\n",
      "Current iteration: 109, mean reward: 0.0\n",
      "Current iteration: 110, mean reward: 0.0\n",
      "Current iteration: 111, mean reward: 0.0\n",
      "Current iteration: 112, mean reward: 0.0625\n",
      "Current iteration: 113, mean reward: 0.0625\n",
      "Current iteration: 114, mean reward: 0.0625\n",
      "Current iteration: 115, mean reward: 0.0\n",
      "Current iteration: 116, mean reward: 0.125\n",
      "Current iteration: 117, mean reward: 0.0\n",
      "Current iteration: 118, mean reward: 0.125\n",
      "Current iteration: 119, mean reward: 0.0\n",
      "Current iteration: 120, mean reward: 0.0\n",
      "Current iteration: 121, mean reward: 0.0\n",
      "Current iteration: 122, mean reward: 0.0625\n",
      "Current iteration: 123, mean reward: 0.0625\n",
      "Current iteration: 124, mean reward: 0.0\n",
      "Current iteration: 125, mean reward: 0.0625\n",
      "Current iteration: 126, mean reward: 0.0625\n",
      "Current iteration: 127, mean reward: 0.0\n",
      "Current iteration: 128, mean reward: 0.0\n",
      "Current iteration: 129, mean reward: 0.0\n",
      "Current iteration: 130, mean reward: 0.0\n",
      "Current iteration: 131, mean reward: 0.0625\n",
      "Current iteration: 132, mean reward: 0.0\n",
      "Current iteration: 133, mean reward: 0.0\n",
      "Current iteration: 134, mean reward: 0.0\n",
      "Current iteration: 135, mean reward: 0.0\n",
      "Current iteration: 136, mean reward: 0.0625\n",
      "Current iteration: 137, mean reward: 0.0\n",
      "Current iteration: 138, mean reward: 0.0\n",
      "Current iteration: 139, mean reward: 0.0\n",
      "Current iteration: 140, mean reward: 0.0\n",
      "Current iteration: 141, mean reward: 0.0\n",
      "Current iteration: 142, mean reward: 0.0\n",
      "Current iteration: 143, mean reward: 0.0625\n",
      "Current iteration: 144, mean reward: 0.0\n",
      "Current iteration: 145, mean reward: 0.0625\n",
      "Current iteration: 146, mean reward: 0.0625\n",
      "Current iteration: 147, mean reward: 0.0\n",
      "Current iteration: 148, mean reward: 0.0\n",
      "Current iteration: 149, mean reward: 0.0\n",
      "Current iteration: 150, mean reward: 0.0625\n",
      "Current iteration: 151, mean reward: 0.0\n",
      "Current iteration: 152, mean reward: 0.0625\n",
      "Current iteration: 153, mean reward: 0.0\n",
      "Current iteration: 154, mean reward: 0.0\n",
      "Current iteration: 155, mean reward: 0.0\n",
      "Current iteration: 156, mean reward: 0.0625\n",
      "Current iteration: 157, mean reward: 0.0\n",
      "Current iteration: 158, mean reward: 0.0\n",
      "Current iteration: 159, mean reward: 0.0625\n",
      "Current iteration: 160, mean reward: 0.0625\n",
      "Current iteration: 161, mean reward: 0.0\n",
      "Current iteration: 162, mean reward: 0.125\n",
      "Current iteration: 163, mean reward: 0.0\n",
      "Current iteration: 164, mean reward: 0.125\n",
      "Current iteration: 165, mean reward: 0.0\n",
      "Current iteration: 166, mean reward: 0.0625\n",
      "Current iteration: 167, mean reward: 0.0\n",
      "Current iteration: 168, mean reward: 0.0625\n",
      "Current iteration: 169, mean reward: 0.0\n",
      "Current iteration: 170, mean reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter(log_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfrozen_lake_tb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(MAX_ITER):\n\u001b[0;32m---> 13\u001b[0m     batch \u001b[39m=\u001b[39m create_batch(env, net, BATCH_SIZE)\n\u001b[1;32m     14\u001b[0m     training_states, training_actions, reward_percentile, mean_reward \u001b[39m=\u001b[39m filter_batch(batch\u001b[39m=\u001b[39mbatch, percentile\u001b[39m=\u001b[39mPERCENTILE)\n\u001b[1;32m     15\u001b[0m     training_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(training_states)\n",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m, in \u001b[0;36mcreate_batch\u001b[0;34m(env, net, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m batch \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[0;32m----> 4\u001b[0m     episode \u001b[39m=\u001b[39m create_episode(env, net)\n\u001b[1;32m      5\u001b[0m     batch\u001b[39m.\u001b[39mappend(episode)\n\u001b[1;32m      7\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m, in \u001b[0;36mcreate_episode\u001b[0;34m(env, net)\u001b[0m\n\u001b[1;32m     11\u001b[0m action_prob \u001b[39m=\u001b[39m sm(net(current_state_tensor))\n\u001b[1;32m     12\u001b[0m action_prob \u001b[39m=\u001b[39m action_prob\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m---> 13\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn, p\u001b[39m=\u001b[39maction_prob)\n\u001b[1;32m     15\u001b[0m next_state, reward, terminated, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action\u001b[39m=\u001b[39maction)\n\u001b[1;32m     16\u001b[0m total_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    env =  DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "    \n",
    "    num_features = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "    net = Net(num_features=num_features, hidden_size=HIDDEN_SIZE, num_actions=num_actions)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(params = net.parameters(), lr = 0.01)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir='frozen_lake_tb')\n",
    "    for i in range(MAX_ITER):\n",
    "        batch = create_batch(env, net, BATCH_SIZE)\n",
    "        training_states, training_actions, reward_percentile, mean_reward = filter_batch(batch=batch, percentile=PERCENTILE)\n",
    "        training_states = torch.FloatTensor(training_states)\n",
    "        training_actions = torch.LongTensor(training_actions)\n",
    "        # Zero the parameter gradients\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(training_states)\n",
    "        outputs = outputs.float()\n",
    "        loss = criterion(outputs, training_actions)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        print('Current iteration: {}, mean reward: {}'.format(i, mean_reward))\n",
    "        \n",
    "        writer.add_scalar(\"loss\", loss.item(), i)\n",
    "        writer.add_scalar(\"reward_bound\", reward_percentile, i)\n",
    "        writer.add_scalar(\"mean_reward\", mean_reward, i)\n",
    "        if mean_reward > 0.8:\n",
    "            break\n",
    "        \n",
    "    writer.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
