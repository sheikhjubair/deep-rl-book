{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "import gym\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100 #Batch size is increased\n",
    "PERCENTILE = 70\n",
    "MAX_ITER = 10000000\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space,\n",
    "                          gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n, )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            0.0, 1.0, shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features:int, hidden_size:int, num_actions:int):\n",
    "        super(Net, self).__init__()\n",
    "        # Define two linear layers\n",
    "        self.linear1 = nn.Linear(in_features=num_features, out_features=hidden_size)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_size, out_features=num_actions)\n",
    "\n",
    "        # Define a ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)  # First linear layer\n",
    "        x = self.relu(x)     # ReLU activation\n",
    "        x = self.linear2(x)  # Second linear layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['total_rewards', 'steps'])\n",
    "episodeStep = namedtuple('episodeStep', field_names=['state', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_episode(env:gym.Env, net:Net):\n",
    "    net.eval()\n",
    "    total_rewards = 0\n",
    "    steps = []\n",
    "    sm = nn.Softmax(dim=0)\n",
    "    \n",
    "    current_state = env.reset()[0]\n",
    "    \n",
    "    while True:\n",
    "        current_state_tensor = torch.FloatTensor(current_state)\n",
    "        action_prob = sm(net(current_state_tensor))\n",
    "        action_prob = action_prob.detach().numpy()\n",
    "        action = np.random.choice(env.action_space.n, p=action_prob)\n",
    "        \n",
    "        next_state, reward, terminated, _, info = env.step(action=action)\n",
    "        total_rewards += reward\n",
    "        current_step = episodeStep(state=current_state, action=action)\n",
    "        steps.append(current_step)\n",
    "        \n",
    "        if terminated:\n",
    "            e = Episode(total_rewards=total_rewards, steps=steps)\n",
    "            return e\n",
    "            \n",
    "        \n",
    "        current_state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(env:gym.Env, net:Net, batch_size:int):\n",
    "    batch = []\n",
    "    for i in range(batch_size):\n",
    "        episode = create_episode(env, net)\n",
    "        batch.append(episode)\n",
    "        \n",
    "    return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    discounted_rewards = list(map(lambda s:s.total_rewards * (GAMMA**len(s.steps)), batch))  #Discounted reward is introduced\n",
    "    reward_percentile = np.percentile(discounted_rewards, percentile)\n",
    "    mean_reward = np.mean(discounted_rewards)\n",
    "    \n",
    "    training_states = []\n",
    "    training_actions = []\n",
    "    new_batch = []\n",
    "    for episode, discounted_rewards in zip(batch, discounted_rewards):\n",
    "        steps = episode.steps\n",
    "        if discounted_rewards< reward_percentile:\n",
    "            continue\n",
    "        training_states.extend(map(lambda step: step.state, steps))\n",
    "        training_actions.extend(map(lambda step: step.action, steps))\n",
    "        new_batch.append(episode)\n",
    "        \n",
    "    return new_batch, training_states, training_actions, reward_percentile, mean_reward\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration: 0, mean reward: 0.0\n",
      "Current iteration: 1, mean reward: 0.0017433922005000006\n",
      "Current iteration: 2, mean reward: 0.005690315934000001\n",
      "Current iteration: 3, mean reward: 0.004267736950500001\n",
      "Current iteration: 4, mean reward: 0.004275123980400001\n",
      "Current iteration: 5, mean reward: 0.0048827824771500005\n",
      "Current iteration: 6, mean reward: 0.005962867475387145\n",
      "Current iteration: 7, mean reward: 0.00521750904096375\n",
      "Current iteration: 8, mean reward: 0.00463778581419\n",
      "Current iteration: 9, mean reward: 0.0045226856728710005\n",
      "Current iteration: 10, mean reward: 0.004111532429882728\n",
      "Current iteration: 11, mean reward: 0.004626427694753251\n",
      "Current iteration: 12, mean reward: 0.004809957168303001\n",
      "Current iteration: 13, mean reward: 0.004808029441995643\n",
      "Current iteration: 14, mean reward: 0.004943619500592367\n",
      "Current iteration: 15, mean reward: 0.004634643281805344\n",
      "Current iteration: 16, mean reward: 0.0043620172064050295\n",
      "Current iteration: 17, mean reward: 0.004353548694843398\n",
      "Current iteration: 18, mean reward: 0.004124414553009535\n",
      "Current iteration: 19, mean reward: 0.004345042222969815\n",
      "Current iteration: 20, mean reward: 0.004414593472017441\n",
      "Current iteration: 21, mean reward: 0.004298158322782642\n",
      "Current iteration: 22, mean reward: 0.004111281873966005\n",
      "Current iteration: 23, mean reward: 0.004161412212550755\n",
      "Current iteration: 24, mean reward: 0.004054993578167525\n",
      "Current iteration: 25, mean reward: 0.004103432671314928\n",
      "Current iteration: 26, mean reward: 0.003951453683488449\n",
      "Current iteration: 27, mean reward: 0.003930114857000355\n",
      "Current iteration: 28, mean reward: 0.004126286141241723\n",
      "Current iteration: 29, mean reward: 0.004383562821397188\n",
      "Current iteration: 30, mean reward: 0.00462415292150454\n",
      "Current iteration: 31, mean reward: 0.004866037126166847\n",
      "Current iteration: 32, mean reward: 0.004718581455676942\n",
      "Current iteration: 33, mean reward: 0.004619530581029767\n",
      "Current iteration: 34, mean reward: 0.004487543993000345\n",
      "Current iteration: 35, mean reward: 0.004382831603963998\n",
      "Current iteration: 36, mean reward: 0.004358614111991998\n",
      "Current iteration: 37, mean reward: 0.004262805792931468\n",
      "Current iteration: 38, mean reward: 0.00433629711785143\n",
      "Current iteration: 39, mean reward: 0.004306342338927645\n",
      "Current iteration: 40, mean reward: 0.004201309598953799\n",
      "Current iteration: 41, mean reward: 0.004133441554161399\n",
      "Current iteration: 42, mean reward: 0.004037315006390204\n",
      "Current iteration: 43, mean reward: 0.004055320235195542\n",
      "Current iteration: 44, mean reward: 0.004055042278442287\n",
      "Current iteration: 45, mean reward: 0.004011648127192377\n",
      "Current iteration: 46, mean reward: 0.004017882679805306\n",
      "Current iteration: 47, mean reward: 0.0039689209191836675\n",
      "Current iteration: 48, mean reward: 0.004009604192786245\n",
      "Current iteration: 49, mean reward: 0.00392941210893052\n",
      "Current iteration: 50, mean reward: 0.0038637667218932633\n",
      "Current iteration: 51, mean reward: 0.003926559043872431\n",
      "Current iteration: 52, mean reward: 0.003965159633044021\n",
      "Current iteration: 53, mean reward: 0.003905025158130092\n",
      "Current iteration: 54, mean reward: 0.0038340247007095448\n",
      "Current iteration: 55, mean reward: 0.003939318140096805\n",
      "Current iteration: 56, mean reward: 0.003870207295533702\n",
      "Current iteration: 57, mean reward: 0.0038702762195762254\n",
      "Current iteration: 58, mean reward: 0.003804678317549509\n",
      "Current iteration: 59, mean reward: 0.003789294232475474\n",
      "Current iteration: 60, mean reward: 0.00376884458650586\n",
      "Current iteration: 61, mean reward: 0.00376429522867512\n",
      "Current iteration: 62, mean reward: 0.003821385610616785\n",
      "Current iteration: 63, mean reward: 0.003828936962013398\n",
      "Current iteration: 64, mean reward: 0.0037868639301565014\n",
      "Current iteration: 65, mean reward: 0.003867373677981554\n",
      "Current iteration: 66, mean reward: 0.0038739005201012325\n",
      "Current iteration: 67, mean reward: 0.0038879541164795243\n",
      "Current iteration: 68, mean reward: 0.003882140062631995\n",
      "Current iteration: 69, mean reward: 0.0038809068609490568\n",
      "Current iteration: 70, mean reward: 0.0038262462009356897\n",
      "Current iteration: 71, mean reward: 0.0038501274442830622\n",
      "Current iteration: 72, mean reward: 0.003909425383265486\n",
      "Current iteration: 73, mean reward: 0.0038565953105186555\n",
      "Current iteration: 74, mean reward: 0.0038129273379788123\n",
      "Current iteration: 75, mean reward: 0.0038193976637948797\n",
      "Current iteration: 76, mean reward: 0.003860360925330012\n",
      "Current iteration: 77, mean reward: 0.003810869118595012\n",
      "Current iteration: 78, mean reward: 0.0037626302689925434\n",
      "Current iteration: 79, mean reward: 0.0037155973906301365\n",
      "Current iteration: 80, mean reward: 0.0036697258179063073\n",
      "Current iteration: 81, mean reward: 0.0036897829420781815\n",
      "Current iteration: 82, mean reward: 0.0036971912451856733\n",
      "Current iteration: 83, mean reward: 0.003696463441310179\n",
      "Current iteration: 84, mean reward: 0.003689894529775942\n",
      "Current iteration: 85, mean reward: 0.0037952451007593367\n",
      "Current iteration: 86, mean reward: 0.0037892615693351167\n",
      "Current iteration: 87, mean reward: 0.003979105193114263\n",
      "Current iteration: 88, mean reward: 0.003988137370719721\n",
      "Current iteration: 89, mean reward: 0.0039992816904867445\n",
      "Current iteration: 90, mean reward: 0.0040363590490632255\n",
      "Current iteration: 91, mean reward: 0.004012627104930493\n",
      "Current iteration: 92, mean reward: 0.004027822353333349\n",
      "Current iteration: 93, mean reward: 0.004040196220978369\n",
      "Current iteration: 94, mean reward: 0.0040343708333996494\n",
      "Current iteration: 95, mean reward: 0.003995565215799927\n",
      "Current iteration: 96, mean reward: 0.004011507049244945\n",
      "Current iteration: 97, mean reward: 0.00413688027131352\n",
      "Current iteration: 98, mean reward: 0.004095093601906312\n",
      "Current iteration: 99, mean reward: 0.004054142665887249\n",
      "Current iteration: 100, mean reward: 0.004085844956589467\n",
      "Current iteration: 101, mean reward: 0.004098981625705218\n",
      "Current iteration: 102, mean reward: 0.004083863996604478\n",
      "Current iteration: 103, mean reward: 0.004069037091140291\n",
      "Current iteration: 104, mean reward: 0.004110331297849228\n",
      "Current iteration: 105, mean reward: 0.00407155458749216\n",
      "Current iteration: 106, mean reward: 0.004091051201523559\n",
      "Current iteration: 107, mean reward: 0.004053171097805748\n",
      "Current iteration: 108, mean reward: 0.0040969639225125936\n",
      "Current iteration: 109, mean reward: 0.004196297941555032\n",
      "Current iteration: 110, mean reward: 0.004232177032081563\n",
      "Current iteration: 111, mean reward: 0.0042255217407326205\n",
      "Current iteration: 112, mean reward: 0.004235157920018173\n",
      "Current iteration: 113, mean reward: 0.004260879606586889\n",
      "Current iteration: 114, mean reward: 0.004257517217747003\n",
      "Current iteration: 115, mean reward: 0.004287611119145736\n",
      "Current iteration: 116, mean reward: 0.004270517684328217\n",
      "Current iteration: 117, mean reward: 0.004306148738183703\n",
      "Current iteration: 118, mean reward: 0.0042992633235855214\n",
      "Current iteration: 119, mean reward: 0.004263436129222308\n",
      "Current iteration: 120, mean reward: 0.004307697666170883\n",
      "Current iteration: 121, mean reward: 0.004312640385231692\n",
      "Current iteration: 122, mean reward: 0.004312916402965114\n",
      "Current iteration: 123, mean reward: 0.004294353259005686\n",
      "Current iteration: 124, mean reward: 0.0042599984329336395\n",
      "Current iteration: 125, mean reward: 0.004256011400721743\n",
      "Current iteration: 126, mean reward: 0.004290666223471966\n",
      "Current iteration: 127, mean reward: 0.004337309034863154\n",
      "Current iteration: 128, mean reward: 0.004335004793590693\n",
      "Current iteration: 129, mean reward: 0.004301658602870765\n",
      "Current iteration: 130, mean reward: 0.004313828440788015\n",
      "Current iteration: 131, mean reward: 0.004298478825672166\n",
      "Current iteration: 132, mean reward: 0.004283360031836256\n",
      "Current iteration: 133, mean reward: 0.004313456818724233\n",
      "Current iteration: 134, mean reward: 0.004316934686733683\n",
      "Current iteration: 135, mean reward: 0.004346758357206453\n",
      "Current iteration: 136, mean reward: 0.004371339939552533\n",
      "Current iteration: 137, mean reward: 0.004439290648146416\n",
      "Current iteration: 138, mean reward: 0.004497150716784585\n",
      "Current iteration: 139, mean reward: 0.004502988283093266\n",
      "Current iteration: 140, mean reward: 0.00447105219597913\n",
      "Current iteration: 141, mean reward: 0.004439565912908854\n",
      "Current iteration: 142, mean reward: 0.004422917978699327\n",
      "Current iteration: 143, mean reward: 0.004446310607326414\n",
      "Current iteration: 144, mean reward: 0.004415646396241405\n",
      "Current iteration: 145, mean reward: 0.004414886298321943\n",
      "Current iteration: 146, mean reward: 0.004384853058197304\n",
      "Current iteration: 147, mean reward: 0.004370682964868242\n",
      "Current iteration: 148, mean reward: 0.0043673508972516765\n",
      "Current iteration: 149, mean reward: 0.004338235224603332\n",
      "Current iteration: 150, mean reward: 0.004328209132816622\n",
      "Current iteration: 151, mean reward: 0.004325222262798091\n",
      "Current iteration: 152, mean reward: 0.004342256660743474\n",
      "Current iteration: 153, mean reward: 0.004395712302563322\n",
      "Current iteration: 154, mean reward: 0.004391595657864428\n",
      "Current iteration: 155, mean reward: 0.004405982795883578\n",
      "Current iteration: 156, mean reward: 0.004431559338350364\n",
      "Current iteration: 157, mean reward: 0.004403511494436754\n",
      "Current iteration: 158, mean reward: 0.004402889863025203\n",
      "Current iteration: 159, mean reward: 0.004375371801381295\n",
      "Current iteration: 160, mean reward: 0.004360983848086669\n",
      "Current iteration: 161, mean reward: 0.004334064194703418\n",
      "Current iteration: 162, mean reward: 0.004307474843815667\n",
      "Current iteration: 163, mean reward: 0.004307713354319489\n",
      "Current iteration: 164, mean reward: 0.004312454312389709\n",
      "Current iteration: 165, mean reward: 0.004318490190025916\n",
      "Current iteration: 166, mean reward: 0.004301618670043544\n",
      "Current iteration: 167, mean reward: 0.00434435587590525\n",
      "Current iteration: 168, mean reward: 0.004318649628118828\n",
      "Current iteration: 169, mean reward: 0.004293245806776953\n",
      "Current iteration: 170, mean reward: 0.004284655453315158\n",
      "Current iteration: 171, mean reward: 0.004325315833830768\n",
      "Current iteration: 172, mean reward: 0.004300314008201687\n",
      "Current iteration: 173, mean reward: 0.004347082870552806\n",
      "Current iteration: 174, mean reward: 0.004322242397006788\n",
      "Current iteration: 175, mean reward: 0.004315514349074364\n",
      "Current iteration: 176, mean reward: 0.004310397110032246\n",
      "Current iteration: 177, mean reward: 0.004304578462598649\n",
      "Current iteration: 178, mean reward: 0.004280530538226591\n",
      "Current iteration: 179, mean reward: 0.004256749813014221\n",
      "Current iteration: 180, mean reward: 0.004262548524727001\n",
      "Current iteration: 181, mean reward: 0.004246550190622307\n",
      "Current iteration: 182, mean reward: 0.004223344998323826\n",
      "Current iteration: 183, mean reward: 0.004220166661176246\n",
      "Current iteration: 184, mean reward: 0.004218296597548266\n",
      "Current iteration: 185, mean reward: 0.00420791693436519\n",
      "Current iteration: 186, mean reward: 0.004224959855832855\n",
      "Current iteration: 187, mean reward: 0.004225383857131616\n",
      "Current iteration: 188, mean reward: 0.0042207731835056375\n",
      "Current iteration: 189, mean reward: 0.004246058583470215\n",
      "Current iteration: 190, mean reward: 0.004246473154854035\n",
      "Current iteration: 191, mean reward: 0.004240077825337294\n",
      "Current iteration: 192, mean reward: 0.004238182110646428\n",
      "Current iteration: 193, mean reward: 0.004219333220230881\n",
      "Current iteration: 194, mean reward: 0.004244878642863186\n",
      "Current iteration: 195, mean reward: 0.004223221098766944\n",
      "Current iteration: 196, mean reward: 0.004228760128722442\n",
      "Current iteration: 197, mean reward: 0.004230074257762092\n",
      "Current iteration: 198, mean reward: 0.004208817603200472\n",
      "Current iteration: 199, mean reward: 0.00420466260657739\n",
      "Current iteration: 200, mean reward: 0.004199356354608846\n",
      "Current iteration: 201, mean reward: 0.004206920061164505\n",
      "Current iteration: 202, mean reward: 0.00421652977657183\n",
      "Current iteration: 203, mean reward: 0.0042150366478131375\n",
      "Current iteration: 204, mean reward: 0.004209958288835812\n",
      "Current iteration: 205, mean reward: 0.0042413065619152455\n",
      "Current iteration: 206, mean reward: 0.004245718136838199\n",
      "Current iteration: 207, mean reward: 0.004238884373511139\n",
      "Current iteration: 208, mean reward: 0.0042487991840006074\n",
      "Current iteration: 209, mean reward: 0.0042319853687800895\n",
      "Current iteration: 210, mean reward: 0.004255476503951748\n",
      "Current iteration: 211, mean reward: 0.004235403501574618\n",
      "Current iteration: 212, mean reward: 0.004224218603392821\n",
      "Current iteration: 213, mean reward: 0.004216357141827101\n",
      "Current iteration: 214, mean reward: 0.0041967461783767425\n",
      "Current iteration: 215, mean reward: 0.004187907905539333\n",
      "Current iteration: 216, mean reward: 0.004224069071590266\n",
      "Current iteration: 217, mean reward: 0.0042108891754713785\n",
      "Current iteration: 218, mean reward: 0.0042377681244418295\n",
      "Current iteration: 219, mean reward: 0.004218505542058003\n",
      "Current iteration: 220, mean reward: 0.004215194586668601\n",
      "Current iteration: 221, mean reward: 0.0042076570697391425\n",
      "Current iteration: 222, mean reward: 0.004210339433241957\n",
      "Current iteration: 223, mean reward: 0.004191543275057842\n",
      "Current iteration: 224, mean reward: 0.004172914193835362\n",
      "Current iteration: 225, mean reward: 0.004154449971738745\n",
      "Current iteration: 226, mean reward: 0.004147346076833856\n",
      "Current iteration: 227, mean reward: 0.004170471316022947\n",
      "Current iteration: 228, mean reward: 0.004191528441273046\n",
      "Current iteration: 229, mean reward: 0.004194337605087827\n",
      "Current iteration: 230, mean reward: 0.004176180299438097\n",
      "Current iteration: 231, mean reward: 0.004158179522285346\n",
      "Current iteration: 232, mean reward: 0.004140333258241204\n",
      "Current iteration: 233, mean reward: 0.004141035560983762\n",
      "Current iteration: 234, mean reward: 0.004123414133064683\n",
      "Current iteration: 235, mean reward: 0.004105942039280511\n",
      "Current iteration: 236, mean reward: 0.00408861738932574\n",
      "Current iteration: 237, mean reward: 0.004071438324664708\n",
      "Current iteration: 238, mean reward: 0.0040864613085928175\n",
      "Current iteration: 239, mean reward: 0.004075062935297319\n",
      "Current iteration: 240, mean reward: 0.004068701121575458\n",
      "Current iteration: 241, mean reward: 0.004051888307023494\n",
      "Current iteration: 242, mean reward: 0.004061103610999371\n",
      "Current iteration: 243, mean reward: 0.004052054088777454\n",
      "Current iteration: 244, mean reward: 0.004035515092496731\n",
      "Current iteration: 245, mean reward: 0.004027480117815632\n",
      "Current iteration: 246, mean reward: 0.004011174530294111\n",
      "Current iteration: 247, mean reward: 0.00399500043944615\n",
      "Current iteration: 248, mean reward: 0.00397895626097448\n",
      "Current iteration: 249, mean reward: 0.003982172311930581\n",
      "Current iteration: 250, mean reward: 0.003966307083596197\n",
      "Current iteration: 251, mean reward: 0.0039683932203072305\n",
      "Current iteration: 252, mean reward: 0.003971913309524133\n",
      "Current iteration: 253, mean reward: 0.003992053970116558\n",
      "Current iteration: 254, mean reward: 0.0039763988565082575\n",
      "Current iteration: 255, mean reward: 0.003960866048475022\n",
      "Current iteration: 256, mean reward: 0.003962203815212473\n",
      "Current iteration: 257, mean reward: 0.003946846436083743\n",
      "Current iteration: 258, mean reward: 0.0039316076467552355\n",
      "Current iteration: 259, mean reward: 0.003916486078883098\n",
      "Current iteration: 260, mean reward: 0.0039014803850942743\n",
      "Current iteration: 261, mean reward: 0.0038865892385862816\n",
      "Current iteration: 262, mean reward: 0.003899132529367888\n",
      "Current iteration: 263, mean reward: 0.0038843630879687673\n",
      "Current iteration: 264, mean reward: 0.003869705114051904\n",
      "Current iteration: 265, mean reward: 0.003855157350465243\n",
      "Current iteration: 266, mean reward: 0.0038674891164444006\n",
      "Current iteration: 267, mean reward: 0.0038675141753009516\n",
      "Current iteration: 268, mean reward: 0.0038531367991845914\n",
      "Current iteration: 269, mean reward: 0.003838865922150574\n",
      "Current iteration: 270, mean reward: 0.0038621842037772137\n",
      "Current iteration: 271, mean reward: 0.0038479849971456803\n",
      "Current iteration: 272, mean reward: 0.003855622840419676\n",
      "Current iteration: 273, mean reward: 0.003841551224213765\n",
      "Current iteration: 274, mean reward: 0.003833039933772878\n",
      "Current iteration: 275, mean reward: 0.003830522057059571\n",
      "Current iteration: 276, mean reward: 0.0038227141007884775\n",
      "Current iteration: 277, mean reward: 0.003815628870889424\n",
      "Current iteration: 278, mean reward: 0.003829218962265484\n",
      "Current iteration: 279, mean reward: 0.003815543180257393\n",
      "Current iteration: 280, mean reward: 0.0038092918213274607\n",
      "Current iteration: 281, mean reward: 0.0038095220095142428\n",
      "Current iteration: 282, mean reward: 0.003796060801000059\n",
      "Current iteration: 283, mean reward: 0.0038051738018371234\n",
      "Current iteration: 284, mean reward: 0.0038191607586762913\n",
      "Current iteration: 285, mean reward: 0.0038058070497298704\n",
      "Current iteration: 286, mean reward: 0.0037925463979886515\n",
      "Current iteration: 287, mean reward: 0.003786560992805931\n",
      "Current iteration: 288, mean reward: 0.003773458705633592\n",
      "Current iteration: 289, mean reward: 0.0037936160276831306\n",
      "Current iteration: 290, mean reward: 0.0037805795464883434\n",
      "Current iteration: 291, mean reward: 0.0037994672817734916\n",
      "Current iteration: 292, mean reward: 0.0038164528111981\n",
      "Current iteration: 293, mean reward: 0.0038034716791872226\n",
      "Current iteration: 294, mean reward: 0.0037905785548509943\n",
      "Current iteration: 295, mean reward: 0.003777772546219741\n",
      "Current iteration: 296, mean reward: 0.003782551148023198\n",
      "Current iteration: 297, mean reward: 0.003786883227743949\n",
      "Current iteration: 298, mean reward: 0.003781869167602652\n",
      "Current iteration: 299, mean reward: 0.0037852061670439765\n",
      "Current iteration: 300, mean reward: 0.0037726307312730663\n",
      "Current iteration: 301, mean reward: 0.0037743924576595796\n",
      "Current iteration: 302, mean reward: 0.0037747218716276997\n",
      "Current iteration: 303, mean reward: 0.003796788358913897\n",
      "Current iteration: 304, mean reward: 0.003784339872491229\n",
      "Current iteration: 305, mean reward: 0.003793622717227859\n",
      "Current iteration: 306, mean reward: 0.00378482977642684\n",
      "Current iteration: 307, mean reward: 0.003802046696308571\n",
      "Current iteration: 308, mean reward: 0.0038081241237937504\n",
      "Current iteration: 309, mean reward: 0.003822458633603749\n",
      "Current iteration: 310, mean reward: 0.003824494786743384\n",
      "Current iteration: 311, mean reward: 0.0038122367906320265\n",
      "Current iteration: 312, mean reward: 0.0038100830180130747\n",
      "Current iteration: 313, mean reward: 0.0037979489956627144\n",
      "Current iteration: 314, mean reward: 0.0038029274470832747\n",
      "Current iteration: 315, mean reward: 0.0038516049554785797\n",
      "Current iteration: 316, mean reward: 0.003851676248647418\n",
      "Current iteration: 317, mean reward: 0.0038395640591862624\n",
      "Current iteration: 318, mean reward: 0.0038373651309784682\n",
      "Current iteration: 319, mean reward: 0.003841595133419052\n",
      "Current iteration: 320, mean reward: 0.003854840312749416\n",
      "Current iteration: 321, mean reward: 0.0038562373058775245\n",
      "Current iteration: 322, mean reward: 0.0038593417593954997\n",
      "Current iteration: 323, mean reward: 0.0038474302107553908\n",
      "Current iteration: 324, mean reward: 0.0038526440349068093\n",
      "Current iteration: 325, mean reward: 0.0038428080353792513\n",
      "Current iteration: 326, mean reward: 0.0038617349134938456\n",
      "Current iteration: 327, mean reward: 0.0038499613314405107\n",
      "Current iteration: 328, mean reward: 0.0038382593213145515\n",
      "Current iteration: 329, mean reward: 0.0038396726933711744\n",
      "Current iteration: 330, mean reward: 0.003847139179994857\n",
      "Current iteration: 331, mean reward: 0.003856159484636278\n",
      "Current iteration: 332, mean reward: 0.0038445794261238557\n",
      "Current iteration: 333, mean reward: 0.003851882827102437\n",
      "Current iteration: 334, mean reward: 0.0038546621888125793\n",
      "Current iteration: 335, mean reward: 0.003866475286523246\n",
      "Current iteration: 336, mean reward: 0.0038569193010704254\n",
      "Current iteration: 337, mean reward: 0.003852276579012513\n",
      "Current iteration: 338, mean reward: 0.0038706987100478743\n",
      "Current iteration: 339, mean reward: 0.003869397124548599\n",
      "Current iteration: 340, mean reward: 0.0038580499189047614\n",
      "Current iteration: 341, mean reward: 0.003846769071188666\n",
      "Current iteration: 342, mean reward: 0.0038409563922314157\n",
      "Current iteration: 343, mean reward: 0.0038565019111202773\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m batch \u001b[39m=\u001b[39m create_batch(env, net, BATCH_SIZE)\n\u001b[1;32m     15\u001b[0m new_batch, training_states, training_actions, reward_percentile, mean_reward \u001b[39m=\u001b[39m filter_batch(batch\u001b[39m=\u001b[39mbatch \u001b[39m+\u001b[39m prev_batch, percentile\u001b[39m=\u001b[39mPERCENTILE)\n\u001b[0;32m---> 16\u001b[0m training_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mFloatTensor(training_states)\n\u001b[1;32m     17\u001b[0m training_actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(training_actions)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Zero the parameter gradients\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    env =  DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "    \n",
    "    num_features = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "    net = Net(num_features=num_features, hidden_size=HIDDEN_SIZE, num_actions=num_actions)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(params = net.parameters(), lr = 0.001)  #Learning rate is decreased\n",
    "    \n",
    "    writer = SummaryWriter(log_dir='frozen_lake_tb')\n",
    "    prev_batch = []\n",
    "    for i in range(MAX_ITER):\n",
    "        batch = create_batch(env, net, BATCH_SIZE)\n",
    "        new_batch, training_states, training_actions, reward_percentile, mean_reward = filter_batch(batch=batch + prev_batch, percentile=PERCENTILE)\n",
    "        training_states = torch.FloatTensor(training_states)\n",
    "        training_actions = torch.LongTensor(training_actions)\n",
    "        # Zero the parameter gradients\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(training_states)\n",
    "        outputs = outputs.float()\n",
    "        loss = criterion(outputs, training_actions)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        print('Current iteration: {}, mean reward: {}'.format(i, mean_reward))\n",
    "        \n",
    "        writer.add_scalar(\"loss\", loss.item(), i)\n",
    "        writer.add_scalar(\"reward_bound\", reward_percentile, i)\n",
    "        writer.add_scalar(\"mean_reward\", mean_reward, i)\n",
    "        \n",
    "        prev_batch = new_batch\n",
    "        if mean_reward > 0.8:\n",
    "            break\n",
    "        \n",
    "    writer.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
